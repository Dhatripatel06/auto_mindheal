import 'dart:async';
import 'dart:io';
import 'package:flutter/foundation.dart';
import 'package:mental_wellness_app/core/services/gemini_adviser_service.dart';
import 'package:mental_wellness_app/core/services/live_speech_transcription_service.dart';
import 'package:mental_wellness_app/core/services/translation_service.dart';
import 'package:mental_wellness_app/core/services/tts_service.dart';
import 'package:mental_wellness_app/features/mood_detection/data/models/audio_emotion_result.dart';
import 'package:mental_wellness_app/features/mood_detection/data/services/wav2vec2_emotion_service.dart';

/// Enhanced Audio Detection Provider with complete pipeline
/// Handles recording, emotion detection, transcription, translation, and AI advice
class AudioDetectionProvider extends ChangeNotifier {
  // Services
  final Wav2Vec2EmotionService _emotionService =
      Wav2Vec2EmotionService.instance;
  final LiveSpeechTranscriptionService _sttService =
      LiveSpeechTranscriptionService();
  final TranslationService _translationService = TranslationService();
  final GeminiAdviserService _geminiService = GeminiAdviserService();
  final TtsService _ttsService = TtsService();

  // State variables
  bool _isInitialized = false;
  bool _isRecording = false;
  bool _isProcessing = false;
  bool _hasRecording = false;

  // Results and data
  AudioEmotionResult? _lastResult;
  String? _friendlyResponse;
  String? _lastError;
  List<double> _audioData = [];
  Duration _recordingDuration = Duration.zero;

  // Transcription and language
  String _liveTranscribedText = "";
  String? _lastRecordedFilePath;
  String _selectedLanguage = 'English';

  // TTS state tracking
  bool _isSpeaking = false;

  // Getters
  bool get isInitialized => _isInitialized;
  bool get isRecording => _isRecording;
  bool get isProcessing => _isProcessing;
  bool get hasRecording => _hasRecording;
  bool get isSpeaking => _isSpeaking;
  AudioEmotionResult? get lastResult => _lastResult;
  String? get friendlyResponse => _friendlyResponse;
  String? get lastError => _lastError;
  List<double> get audioData => _audioData;
  Duration get recordingDuration => _recordingDuration;
  String get selectedLanguage => _selectedLanguage;
  String get liveTranscribedText => _liveTranscribedText;
  String? get audioFilePath => _lastRecordedFilePath;

  // Language mappings
  String get currentLangCode => _getLangCode(_selectedLanguage);
  String get currentLocaleId => _getLocaleId(_selectedLanguage);

  /// Initialize the audio detection system
  Future<void> initialize() async {
    if (_isInitialized) return;

    try {
      print('ЁЯЪА Initializing Audio Detection Provider...');

      // Initialize emotion detection service
      await _emotionService.initialize();

      // Setup STT listener
      _sttService.addListener(() {
        _liveTranscribedText = _sttService.liveWords;
        if (_mounted) notifyListeners();
      });

      // Setup audio data stream listener
      _emotionService.audioDataStream.listen((data) {
        _audioData = data;
        if (_mounted) notifyListeners();
      });

      // Setup recording duration listener
      _emotionService.recordingDurationStream.listen((duration) {
        _recordingDuration = duration;
        if (_mounted) notifyListeners();
      });

      // Setup TTS state listener
      _ttsService.onStateChanged = (state) {
        _isSpeaking = (state == TtsState.playing);
        if (_mounted) notifyListeners();
      };

      _isInitialized = true;
      print('тЬЕ Audio Detection Provider initialized successfully');
    } catch (e) {
      _lastError = "Initialization failed: $e";
      print('тЭМ Audio Detection Provider initialization failed: $e');
      rethrow;
    }
  }

  /// Set the selected language
  void setLanguage(String language) {
    if (_isRecording || _isProcessing) return;

    _selectedLanguage = language;
    print('ЁЯМР Language changed to: $language');
    notifyListeners();
  }

  /// Start audio recording
  Future<void> startRecording() async {
    if (_isRecording) return;

    try {
      print('ЁЯОЩя╕П Starting audio recording...');
      _clearState();
      _isRecording = true;
      _isProcessing = false;

      // Start recording with emotion service
      await _emotionService.startRecording();

      // Start speech recognition
      try {
        await _sttService.startListening(currentLocaleId);
      } catch (e) {
        print("тЪая╕П STT Warning: $e");
        // Continue even if STT fails
      }

      notifyListeners();
      print('тЬЕ Recording started successfully');
    } catch (e) {
      _lastError = "Could not start recording: $e";
      _isRecording = false;
      print('тЭМ Failed to start recording: $e');
      notifyListeners();
      rethrow;
    }
  }

  /// Stop recording and process the audio
  Future<void> stopRecording() async {
    if (!_isRecording) return;

    print('ЁЯЫС Stopping recording...');
    _isRecording = false;
    _isProcessing = true;
    notifyListeners();

    try {
      // Stop services
      File? audioFile = await _emotionService.stopRecording();
      await _sttService.stopListening();

      if (audioFile != null) {
        _hasRecording = true;
        _lastRecordedFilePath = audioFile.path;

        // Get transcribed text
        String userText = _sttService.finalText;
        if (userText.isEmpty) userText = _liveTranscribedText;
        if (userText.isEmpty) userText = "(No speech detected)";
        _liveTranscribedText = userText;

        print('ЁЯУЭ Transcribed text: "$userText"');

        // Process the complete pipeline
        await _processAudioPipeline(audioFile, userText);
      } else {
        throw Exception("No audio file received from recording service");
      }
    } catch (e) {
      _lastError = "Processing Error: $e";
      print('тЭМ Processing failed: $e');
    } finally {
      _isProcessing = false;
      if (_mounted) notifyListeners();
    }
  }

  /// Complete audio processing pipeline
  Future<void> _processAudioPipeline(
      File audioFile, String originalText) async {
    final startTime = DateTime.now();

    try {
      print('ЁЯФД Processing audio pipeline...');

      // Step 1: Emotion Detection from Audio
      print('ЁЯОп Detecting emotion from audio...');
      final emotionResult = await _emotionService.analyzeAudio(audioFile);

      if (emotionResult.hasError) {
        throw Exception("Emotion detection failed: ${emotionResult.error}");
      }

      String detectedEmotion = emotionResult.emotion;
      print(
          'ЁЯШК Detected emotion: $detectedEmotion (${(emotionResult.confidence * 100).toInt()}%)');

      // Step 2: Translation (if needed)
      String englishText = originalText;
      String? translatedText;

      if (currentLangCode != 'en' && originalText != "(No speech detected)") {
        print('ЁЯМР Translating to English...');
        try {
          englishText = await _translationService.translate(originalText,
              from: currentLangCode, to: 'en');
          translatedText = englishText;
          print('тЬЕ Translation: "$englishText"');
        } catch (e) {
          print('тЪая╕П Translation failed, using original text: $e');
          englishText = originalText;
        }
      }

      // Step 3: Get AI advice from Gemini
      print('ЁЯдЦ Getting AI advice...');
      String englishAdvice;
      try {
        englishAdvice = await _geminiService.getConversationalAdvice(
          userSpeech: englishText,
          detectedEmotion: detectedEmotion,
          language: 'English',
        );
        print('ЁЯТб Received advice: "$englishAdvice"');
      } catch (e) {
        print('тЪая╕П Gemini failed, using fallback advice: $e');
        englishAdvice = _getFallbackAdvice(detectedEmotion, originalText);
      }

      // Step 4: Translate advice back (if needed)
      String finalAdvice = englishAdvice;
      if (currentLangCode != 'en') {
        print('ЁЯМР Translating advice back to user language...');
        try {
          finalAdvice = await _translationService.translate(englishAdvice,
              from: 'en', to: currentLangCode);
          print('тЬЕ Translated advice: "$finalAdvice"');
        } catch (e) {
          print('тЪая╕П Advice translation failed, using English: $e');
          finalAdvice = englishAdvice;
        }
      }

      // Step 5: Create enhanced result
      final processingTime =
          DateTime.now().difference(startTime).inMilliseconds;

      _lastResult = AudioEmotionResult.success(
        emotion: emotionResult.emotion,
        confidence: emotionResult.confidence,
        allEmotions: emotionResult.allEmotions,
        transcribedText: originalText,
        originalLanguage: _selectedLanguage,
        translatedText: translatedText,
        audioFilePath: audioFile.path,
        audioDuration: _recordingDuration,
        processingTimeMs: processingTime,
      );

      _friendlyResponse = finalAdvice;
      _clearError();

      print('тЬЕ Audio pipeline completed successfully');

      // Step 6: Speak the advice
      await speakAdvice(finalAdvice);
    } catch (e) {
      print("тЭМ Pipeline failed: $e");
      _lastError = "Could not analyze: $e";
      _lastResult = AudioEmotionResult.error(
        "Analysis failed: $e",
        language: _selectedLanguage,
        audioPath: audioFile.path,
      );
    }
  }

  /// Analyze uploaded audio file
  Future<void> analyzeAudioFile(File audioFile) async {
    if (_isRecording || _isProcessing) return;

    print('ЁЯУБ Analyzing uploaded audio file: ${audioFile.path}');
    _clearState();
    _isProcessing = true;
    _hasRecording = true;
    _lastRecordedFilePath = audioFile.path;
    _liveTranscribedText = "(Uploaded File - Speech detection in progress...)";
    notifyListeners();

    try {
      // For uploaded files, we don't have real-time transcription,
      // so we'll use a placeholder and focus on emotion detection
      await _processAudioPipeline(
          audioFile, "I uploaded an audio file for analysis.");
    } catch (e) {
      _lastError = "File analysis failed: $e";
      print('тЭМ File analysis failed: $e');
    } finally {
      _isProcessing = false;
      notifyListeners();
    }
  }

  /// Play the last recorded audio
  Future<void> playLastRecording() async {
    if (_lastRecordedFilePath == null ||
        !File(_lastRecordedFilePath!).existsSync()) {
      _lastError = "No recording to play";
      notifyListeners();
      return;
    }

    try {
      print('ЁЯФК Playing last recording...');
      // Implementation depends on your audio player
      // This is a placeholder - you might want to use just_audio or similar
      print(
          'ЁЯУ▒ Audio playback not implemented - file at: $_lastRecordedFilePath');
    } catch (e) {
      _lastError = "Playback failed: $e";
      print('тЭМ Playback failed: $e');
      notifyListeners();
    }
  }

  /// Speak the AI advice
  Future<void> speakAdvice([String? customText]) async {
    final textToSpeak = customText ?? _friendlyResponse;
    if (textToSpeak == null || textToSpeak.isEmpty) return;

    try {
      print('ЁЯФК Speaking advice in $currentLocaleId...');
      await _ttsService.speak(textToSpeak, currentLocaleId);
    } catch (e) {
      print('тЪая╕П TTS failed: $e');
    }
  }

  /// Stop TTS if currently speaking
  Future<void> stopSpeaking() async {
    if (_isSpeaking) {
      await _ttsService.stop();
    }
  }

  /// Clear all state and results
  void _clearState() {
    _lastResult = null;
    _friendlyResponse = null;
    _clearError();
    _audioData = [];
    _recordingDuration = Duration.zero;
    _liveTranscribedText = "";
  }

  /// Clear error state
  void _clearError() {
    _lastError = null;
  }

  /// Public methods to clear results
  void clearResults() {
    _clearState();
    notifyListeners();
  }

  void clearRecording() {
    _clearState();
    _hasRecording = false;
    _lastRecordedFilePath = null;
    notifyListeners();
  }

  /// Helper methods for language codes
  String _getLangCode(String language) {
    switch (language) {
      case 'рд╣рд┐рдВрджреА':
        return 'hi';
      case 'ркЧрлБркЬрк░рк╛ркдрлА':
        return 'gu';
      default:
        return 'en';
    }
  }

  String _getLocaleId(String language) {
    switch (language) {
      case 'рд╣рд┐рдВрджреА':
        return 'hi_IN';
      case 'ркЧрлБркЬрк░рк╛ркдрлА':
        return 'gu_IN';
      default:
        return 'en_US';
    }
  }

  /// Fallback advice when Gemini fails
  String _getFallbackAdvice(String emotion, String userText) {
    if (_selectedLanguage == 'рд╣рд┐рдВрджреА') {
      switch (emotion.toLowerCase()) {
        case 'happy':
          return "рдЦреБрд╢реА рдХреА рдпрд╣ рднрд╛рд╡рдирд╛ рдмрд╣реБрдд рдЕрдЪреНрдЫреА рд╣реИ! рдЗрд╕ рдЦреБрд╢реА рдХреЛ рдЕрдкрдиреЗ рджреЛрд╕реНрддреЛрдВ рдХреЗ рд╕рд╛рде рд╕рд╛рдЭрд╛ рдХрд░реЗрдВред";
        case 'sad':
          return "рдореИрдВ рд╕рдордЭ рд╕рдХрддрд╛ рд╣реВрдБ рдХрд┐ рдЖрдк рдЙрджрд╛рд╕ рд╣реИрдВред рдЧрд╣рд░реА рд╕рд╛рдВрд╕ рд▓реЗрдВ, рдпрд╣ рд╕рдордп рднреА рдмреАрдд рдЬрд╛рдПрдЧрд╛ред";
        case 'angry':
          return "рдЧреБрд╕реНрд╕реЗ рдореЗрдВ рдЧрд╣рд░реА рд╕рд╛рдВрд╕ рд▓реЗрдВ рдФрд░ рдЕрдкрдиреЗ рдЖрдк рдХреЛ рд╢рд╛рдВрдд рд░рдЦрдиреЗ рдХреА рдХреЛрд╢рд┐рд╢ рдХрд░реЗрдВред";
        default:
          return "рдЖрдкрдХреА рднрд╛рд╡рдирд╛рдПрдВ рд╕рд╛рдорд╛рдиреНрдп рд╣реИрдВред рдзреИрд░реНрдп рд░рдЦреЗрдВ, рдЖрдк рдЕрдХреЗрд▓реЗ рдирд╣реАрдВ рд╣реИрдВред";
      }
    } else if (_selectedLanguage == 'ркЧрлБркЬрк░рк╛ркдрлА') {
      switch (emotion.toLowerCase()) {
        case 'happy':
          return "ркЖ ркЦрлБрк╢рлАркирлА рк▓рк╛ркЧркгрлА ркЦрлВркм рк╕рк░рк╕ ркЫрлЗ! ркЖ ркЖркиркВркжркирлЗ ркдркорк╛рк░рк╛ ркорк┐ркдрлНрк░рлЛ рк╕рк╛ркерлЗ рк╡рк╣рлЗркВркЪрлЛред";
        case 'sad':
          return "рк╣рлБркВ рк╕ркоркЬрлА рк╢ркХрлБркВ ркЫрлБркВ ркХрлЗ ркдркорлЗ ркЙркжрк╛рк╕ ркЫрлЛред ркКркВркбрлЛ рк╢рлНрк╡рк╛рк╕ рк▓рлЛ, ркЖ рк╕ркоркп рккркг рккрк╕рк╛рк░ ркеркИ ркЬрк╢рлЗред";
        case 'angry':
          return "ркЧрлБрк╕рлНрк╕рк╛ркорк╛ркВ ркКркВркбрлЛ рк╢рлНрк╡рк╛рк╕ рк▓рлЛ ркЕркирлЗ ркдркорк╛рк░рлА ркЬрк╛ркдркирлЗ рк╢рк╛ркВркд рк░рк╛ркЦрк╡рк╛ркирлЛ рккрлНрк░ркпрк╛рк╕ ркХрк░рлЛред";
        default:
          return "ркдркорк╛рк░рлА рк▓рк╛ркЧркгрлАркУ рк╕рк╛ркорк╛ркирлНркп ркЫрлЗ. ркзрлАрк░ркЬ рк░рк╛ркЦрлЛ, ркдркорлЗ ркПркХрк▓рк╛ ркиркерлАред";
      }
    } else {
      switch (emotion.toLowerCase()) {
        case 'happy':
          return "What a wonderful feeling! Enjoy this moment and share it with people you care about.";
        case 'sad':
          return "I understand you're feeling down. Take deep breaths - this feeling will pass.";
        case 'angry':
          return "I can sense your frustration. Try taking deep breaths and finding a calm space.";
        case 'fear':
          return "You're stronger than you know. Try the 5-4-3-2-1 grounding technique to center yourself.";
        default:
          return "Your feelings are valid. Remember, you have the strength to navigate through this.";
      }
    }
  }

  // Widget lifecycle management
  bool _mounted = true;

  @override
  void dispose() {
    _mounted = false;
    _ttsService.dispose();
    _emotionService.dispose();
    super.dispose();
  }
}
