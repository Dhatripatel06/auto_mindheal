import 'dart:async';
import 'dart:io';
import 'package:flutter/foundation.dart';
import 'package:mental_wellness_app/core/services/gemini_adviser_service.dart';
import 'package:mental_wellness_app/core/services/live_speech_transcription_service.dart';
import 'package:mental_wellness_app/core/services/translation_service.dart';
import 'package:mental_wellness_app/core/services/tts_service.dart';
import 'package:mental_wellness_app/features/mood_detection/data/models/emotion_result.dart';
import 'package:mental_wellness_app/features/mood_detection/data/services/wav2vec2_emotion_service.dart';

class AudioDetectionProvider extends ChangeNotifier {
  final Wav2Vec2EmotionService _emotionService = Wav2Vec2EmotionService.instance;
  final LiveSpeechTranscriptionService _sttService = LiveSpeechTranscriptionService();
  final TranslationService _translationService = TranslationService();
  final GeminiAdviserService _geminiService = GeminiAdviserService();
  final TtsService _ttsService = TtsService();

  bool _isInitialized = false;
  bool _isRecording = false;
  bool _isProcessing = false;
  bool _hasRecording = false;
  
  EmotionResult? _lastResult;
  String? _friendlyResponse;
  String? _lastError;
  List<double> _audioData = [];
  Duration _recordingDuration = Duration.zero;
  
  String _liveTranscribedText = "";
  String? _lastRecordedFilePath; 
  String _selectedLanguage = 'English';

  bool get isRecording => _isRecording;
  bool get isProcessing => _isProcessing;
  bool get isInitialized => _isInitialized;
  bool get hasRecording => _hasRecording || _lastRecordedFilePath != null;
  String? get lastError => _lastError;
  String? get error => _lastError;
  EmotionResult? get lastResult => _lastResult;
  String? get friendlyResponse => _friendlyResponse;
  List<double> get audioData => _audioData;
  Duration get recordingDuration => _recordingDuration;
  String get selectedLanguage => _selectedLanguage;
  String get liveTranscribedText => _liveTranscribedText;
  String? get audioFilePath => _lastRecordedFilePath;

  // Get the user speech text for advice (formatted)
  String get userSpeechForAdvice {
    if (_liveTranscribedText.isNotEmpty && 
        !_liveTranscribedText.startsWith("(") && 
        _liveTranscribedText.trim().isNotEmpty) {
      return _liveTranscribedText.trim();
    }
    return "";
  }

  // Language Code mapping
  String get currentLangCode => _selectedLanguage == '‡§π‡§ø‡§Ç‡§¶‡•Ä' ? 'hi' : (_selectedLanguage == '‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä' ? 'gu' : 'en');
  String get currentLocaleId => _selectedLanguage == '‡§π‡§ø‡§Ç‡§¶‡•Ä' ? 'hi_IN' : (_selectedLanguage == '‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä' ? 'gu_IN' : 'en_US');

  Future<void> initialize() async {
    if (_isInitialized) return;
    
    // Listeners
    _sttService.addListener(() {
      _liveTranscribedText = _sttService.liveWords;
      if (_mounted) notifyListeners();
    });
    
    _emotionService.audioDataStream.listen((data) {
      _audioData = data;
      if (_mounted) notifyListeners();
    });

    _emotionService.recordingDurationStream.listen((duration) {
      _recordingDuration = duration;
      if (_mounted) notifyListeners();
    });
    
    _isInitialized = true;
  }

  void setLanguage(String language) {
    _selectedLanguage = language;
    notifyListeners();
  }

  Future<void> startRecording() async {
    if (_isRecording) return;
    _clearState();
    _isRecording = true;
    _isProcessing = false;
    
    try {
      await _emotionService.startRecording();
      try { 
        await _sttService.startListening(currentLocaleId); 
      } catch (e) { 
        print("STT Warning: $e"); 
      }
      notifyListeners();
    } catch (e) {
      _lastError = "Could not start: $e";
      _isRecording = false;
      notifyListeners();
    }
  }

  Future<void> stopRecording() async {
    if (!_isRecording) return;
    _isRecording = false;
    _isProcessing = true;
    notifyListeners();

    try {
      // 1. Stop Audio
      File? audioFile = await _emotionService.stopRecording();
      await _sttService.stopListening();
      
      if (audioFile != null) {
        _hasRecording = true;
        _lastRecordedFilePath = audioFile.path;
        
        // 2. Get Text
        String userText = _sttService.finalText;
        if (userText.isEmpty) userText = _liveTranscribedText;
        if (userText.isEmpty) userText = "(No speech detected)";
        _liveTranscribedText = userText;

        // 3. Run Full Pipeline
        print('üöÄ About to start friend pipeline...');
        await _processFriendPipeline(audioFile, userText);
        print('‚úÖ Friend pipeline completed successfully');
      }
    } catch (e) {
      _lastError = "Processing Error: $e";
      print('‚ùå stopRecording failed: $e');
    } finally {
      _isProcessing = false;
      if (_mounted) notifyListeners();
    }
  }

  // üî• THE REALITY LOGIC üî•
  Future<void> _processFriendPipeline(File audioFile, String originalText) async {
    try {
      print('üîÑ Starting emotion detection pipeline...');
      print('üìÅ Audio file: ${audioFile.path}');
      print('üìù Original text: "$originalText"');
      
      // A. Detect Tone (Emotion) from Audio
      print('üéØ Calling emotion analysis...');
      _lastResult = await _emotionService.analyzeAudio(audioFile);
      String detectedEmotion = _lastResult?.emotion ?? 'neutral';
      
      print('üòä Emotion detected: $detectedEmotion (confidence: ${_lastResult?.confidence})');
      if (_lastResult?.hasError == true) {
        print('‚ùå Emotion analysis error: ${_lastResult?.error}');
      }
      
      // B. Translate Input to English (if needed)
      String englishText = originalText;
      if (currentLangCode != 'en') {
        englishText = await _translationService.translate(originalText, from: currentLangCode, to: 'en');
      }

      // C. Consult Gemini (The Friend)
      // It sees: "I am tired" (Text) + "SAD" (Tone)
      String englishAdvice = await _geminiService.getConversationalAdvice(
        userSpeech: englishText,
        detectedEmotion: detectedEmotion,
        language: 'English', // Get English response first for stability
      );

      // D. Translate Response back to User Language
      String finalResponse = englishAdvice;
      if (currentLangCode != 'en') {
        finalResponse = await _translationService.translate(englishAdvice, from: 'en', to: currentLangCode);
      }

      _friendlyResponse = finalResponse;
      
      // E. Speak it out
      await _ttsService.speak(finalResponse, currentLocaleId);

    } catch (e) {
      print("Pipeline Failed: $e");
      _lastError = "Could not analyze: $e";
    }
  }
  
  // Helper for file uploads
  Future<void> analyzeAudioFile(File audioFile) async {
    _clearState();
    _isProcessing = true;
    _hasRecording = true;
    _lastRecordedFilePath = audioFile.path;
    _liveTranscribedText = "(Uploaded File)";
    notifyListeners();

    await _processFriendPipeline(audioFile, "I uploaded an audio file.");
    _isProcessing = false;
    notifyListeners();
  }

  void _clearState() {
    _lastResult = null;
    _friendlyResponse = null;
    _lastError = null;
    _audioData = [];
    _recordingDuration = Duration.zero;
    _liveTranscribedText = "";
  }
  
  void clearResults() => _clearState();
  void clearRecording() => _clearState();

  /// Speak the provided advice text
  Future<void> speakAdvice([String? adviceText]) async {
    final textToSpeak = adviceText ?? _friendlyResponse;
    if (textToSpeak == null || textToSpeak.isEmpty) {
      print('No advice text to speak');
      return;
    }

    try {
      print('üîä Speaking advice: "$textToSpeak"');
      await _ttsService.speak(textToSpeak, currentLocaleId);
    } catch (e) {
      print('‚ö†Ô∏è TTS failed: $e');
      _lastError = "Speech failed: $e";
      if (_mounted) notifyListeners();
    }
  }

  /// Play the last recorded audio file
  Future<void> playLastRecording() async {
    if (_lastRecordedFilePath == null) {
      _lastError = 'No recording available to play';
      notifyListeners();
      return;
    }

    try {
      // Simple playback implementation - could be enhanced with audio player
      print('Playing recording: $_lastRecordedFilePath');
      // TODO: Implement actual audio playback if needed
      // For now, just log the attempt
    } catch (e) {
      _lastError = 'Failed to play recording: $e';
      notifyListeners();
    }
  }

  /// Get fresh advice from Gemini for the current emotion
  Future<String> getFreshAdvice([String? customText]) async {
    if (_lastResult == null) {
      throw Exception('No emotion result available');
    }

    final textToAnalyze = customText ?? 
                         (liveTranscribedText.isNotEmpty ? liveTranscribedText : "I shared an audio recording");
    
    try {
      final advice = await _geminiService.getConversationalAdvice(
        userSpeech: textToAnalyze,
        detectedEmotion: _lastResult!.emotion,
        language: _selectedLanguage,
      );
      
      // Update the friendly response with fresh advice
      _friendlyResponse = advice;
      if (_mounted) notifyListeners();
      
      return advice;
    } catch (e) {
      print('Failed to get fresh advice: $e');
      throw Exception('Failed to get advice: $e');
    }
  }

  bool _mounted = true;
  @override
  void dispose() {
    _mounted = false;
    super.dispose();
  }
}